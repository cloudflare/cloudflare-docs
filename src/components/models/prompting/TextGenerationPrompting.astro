---
import { z } from "astro:content";
import { Code } from "@astrojs/starlight/components";

type Props = z.infer<typeof props>;

const props = z.object({
	lora: z.boolean(),
});

const { lora } = props.parse(Astro.props);

const scopedExampleOne = `
{
  messages: [
    { role: "system", content: "you are a very funny comedian and you like emojis" },
    { role: "user", content: "tell me a joke about cloudflare" },
  ],
};
`;

const scopedExampleTwo = `
{
  messages: [
    { role: "system", content: "you are a professional computer science assistant" },
    { role: "user", content: "what is WASM?" },
    { role: "assistant", content: "WASM (WebAssembly) is a binary instruction format that is designed to be a platform-agnostic" },
    { role: "user", content: "does Python compile to WASM?" },
    { role: "assistant", content: "No, Python does not directly compile to WebAssembly" },
    { role: "user", content: "what about Rust?" },
  ],
};
`;

const unscopedExampleOne = `
{
  prompt: "tell me a joke about cloudflare";
}
`;

const unscopedExampleTwo = `
{
  prompt: "<s>[INST]comedian[/INST]</s>\n[INST]tell me a joke about cloudflare[/INST]",
  raw: true
};
`;
---

<h2>Prompting</h2>

<p>
	Part of getting good results from text generation models is asking questions
	correctly. LLMs are usually trained with specific predefined templates, which
	should then be used with the model's tokenizer for better results when doing
	inference tasks
</p>

{
	lora ? (
		<p>We recommend using unscoped prompts for inference with LoRA.</p>
	) : (
		<p>There are two ways to prompt text generation models with Workers AI:</p>
	)
}

{
	!lora && (
		<>
			<h3>Scoped prompts</h3>

			<p>
				This is the <strong>recommended</strong> method. With scoped prompts,
				Workers AI takes the burden of knowing and using different chat
				templates for different models and provides a unified interface to
				developers when building prompts and creating text generation tasks.
			</p>

			<p>
				Scoped prompts are a list of messages. Each message defines two keys:
				the role and the content.
			</p>

			<p>Typically, the role can be one of three options:</p>

			<ul>
				<li>
					<p>
						<strong>system</strong> - System messages define the AI's
						personality. You can use them to set rules and how you expect the AI
						to behave.
					</p>
				</li>
				<li>
					<p>
						<strong>user</strong> - User messages are where you actually query
						the AI by providing a question or a conversation.
					</p>
				</li>
				<li>
					<p>
						<strong>assistant</strong> - Assistant messages hint to the AI about
						the desired output format. Not all models support this role.
					</p>
				</li>
			</ul>

			<p>
				OpenAI has a{" "}
				<a href="https://docs.airops.com/docs/llm-step#openai-chat-model-specifications">
					good explanation
				</a>{" "}
				of how they use these roles with their GPT models. Even though chat
				templates are flexible, other text generation models tend to follow the
				same conventions.
			</p>

			<p>
				Here's an input example of a scoped prompt using system and user roles:
			</p>

			<Code code={scopedExampleOne} lang="js" />

			<p>
				Here's a better example of a chat session using multiple iterations
				between the user and the assistant.
			</p>

			<Code code={scopedExampleTwo} lang="js" />

			<p>
				Note that different LLMs are trained with different templates for
				different use cases. While Workers AI tries its best to abstract the
				specifics of each LLM template from the developer through a unified API,
				you should always refer to the model documentation for details (we
				provide links in the table above.) For example, instruct models like
				Codellama are fine-tuned to respond to a user-provided instruction,
				while chat models expect fragments of dialogs as input.
			</p>
		</>
	)
}

<h3>Unscoped prompts</h3>

<p>
	You can use unscoped prompts to send a single question to the model without
	worrying about providing any context. Workers AI will automatically convert
	your <code>prompt</code> input to a reasonable default scoped prompt internally
	so that you get the best possible prediction.
</p>

<Code code={unscopedExampleOne} lang="js" />

<p>
	You can also use unscoped prompts to construct the model chat template
	manually. In this case, you can use the raw parameter. Here's an input example
	of a <a href="https://docs.mistral.ai/models/#chat-template">Mistral</a> chat template
	prompt:
</p>

<Code code={unscopedExampleTwo} lang="js" />
